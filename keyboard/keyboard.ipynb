{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keyboard(frame, alpha=0.5):\n",
    "    overlay = frame.copy()\n",
    "    origin_x, origin_y = 50, 50\n",
    "    key_width, key_height = 60, 60\n",
    "    padding = 10\n",
    "    rows = [\"1234567890-=\", \"QWERTYUIOP[]\\\\\", \"ASDFGHJKL;'\", \"ZXCVBNM,./\"]\n",
    "    key_positions = []  # Store key positions for collision detection\n",
    "    key_color = (0, 0, 200)\n",
    "    text_color = (255, 255, 255)\n",
    "    border_color = (255, 255, 255)\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        x = origin_x\n",
    "        for key in row:\n",
    "            top_left = (x, origin_y + i * (key_height + padding))\n",
    "            bottom_right = (x + key_width, origin_y + i * (key_height + padding) + key_height)\n",
    "            # Store the key position and its label\n",
    "            key_positions.append((key, top_left, bottom_right))\n",
    "\n",
    "            cv2.rectangle(overlay, top_left, bottom_right, key_color, -1)\n",
    "            cv2.rectangle(overlay, top_left, bottom_right, border_color, 2)\n",
    "            cv2.putText(overlay, key, (x + 15, origin_y + i * (key_height + padding) + int(key_height/2) + 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 1, cv2.LINE_AA)\n",
    "            x += key_width + padding\n",
    "\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "    return key_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "# Create a gesture recognizer instance with the live stream mode:\n",
    "recognized_gesture = \"\"\n",
    "landmarks = []\n",
    "def print_result(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    global recognized_gesture\n",
    "    global landmarks\n",
    "    #print(result)\n",
    "    if result.gestures:\n",
    "        #print(\"Gestures detected:\")\n",
    "        # Take the gesture with the highest confidence\n",
    "        recognized_gesture = \"\"\n",
    "        for gesture in result.gestures:\n",
    "            #print(f\"Gesture: {gesture[0].category_name} ({gesture[0].score:.2f})\")\n",
    "            # Append each gesture to the recognized_gesture variable\n",
    "            recognized_gesture += f\"{gesture[0].category_name} ({gesture[0].score:.2f}), \"\n",
    "    else:\n",
    "        recognized_gesture = \"NOT FOUND\"\n",
    "    if result.hand_landmarks:\n",
    "        landmarks = result.hand_landmarks\n",
    "    #if result.hand_landmarks:\n",
    "    #    hand_landmarks = result.hand_landmarks[0].landmark  # Take the first hand detected\n",
    "    #else:\n",
    "    #    hand_landmarks = []\n",
    "    #print('gesture recognition result: {}'.format(result))\n",
    "\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='C:\\\\Users\\\\Marco\\\\Desktop\\\\GeReco\\\\gesture_recognizer.task'),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    num_hands=2,\n",
    "    result_callback=print_result)\n",
    "\n",
    "# Initialize MediaPipe and OpenCV\n",
    "mp_image = mp.Image\n",
    "mp_image_format = mp.ImageFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TIME_THRESHOLD = 2\n",
    "key_timer = None\n",
    "\n",
    "def is_finger_over_key(finger_x, finger_y, key_positions):\n",
    "    global key_timer\n",
    "    for key, top_left, bottom_right in key_positions:\n",
    "        if top_left[0] <= finger_x <= bottom_right[0] and top_left[1] <= finger_y <= bottom_right[1]:\n",
    "            if key_timer is None:\n",
    "                key_timer = time.time()\n",
    "                print(f\"Start timer for key: {key}, start_time: {key_timer}\")\n",
    "            #     print(f\"Start timer for key: {key}, start_time: {time.time()}\")\n",
    "            #     start_time = time.time()\n",
    "            elif time.time() - key_timer >= 2:\n",
    "                print(f\"Finger over key: {key}\")\n",
    "                key_timer = None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_TIP = 8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "    \n",
    "# Set the desired resolution\n",
    "frame_width = 1280  # Width in pixels\n",
    "frame_height = 720  # Height in pixels\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\n",
    "# Initialize the MediaPipe Hands module\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "        \n",
    "        start_time = time.time()  # Tempo di riferimento iniziale\n",
    "        # Convert the frame (OpenCV image) to MediaPipe's Image object\n",
    "        numpy_frame_from_opencv = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        mp_image_object = mp_image(image_format=mp_image_format.SRGB, data=numpy_frame_from_opencv)\n",
    "\n",
    "\n",
    "        frame_timestamp_ms = int((time.time() - start_time) * 1000)\n",
    "\n",
    "        recognizer.recognize_async(mp_image_object, timestamp_ms=frame_timestamp_ms)\n",
    "\n",
    "                # Overlay the recognized gesture on the frame\n",
    "        cv2.putText(frame, f\"Gesture: {recognized_gesture}\", (10, 700), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                # Draw hand landmarks on the frame\n",
    "\n",
    "        if landmarks:\n",
    "            for it, hand in enumerate(landmarks):\n",
    "                cv2.putText(frame, f\"Index Tip Position Y: {landmarks[it][INDEX_TIP].y * frame_height:.2f}, X: {landmarks[it][INDEX_TIP].x * frame_width:.2f}\", (10, 600 + it * 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                for landmark in hand:\n",
    "                    #print(f\"Landmark: {landmark.x}, {landmark.y}, {landmark.z}\")\n",
    "                    x = int(landmark.x * frame.shape[1])\n",
    "                    y = int(landmark.y * frame.shape[0])\n",
    "                    cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "        \n",
    "         # Draw connections\n",
    "        connections = [\n",
    "            (0, 1), (1, 2), (2, 3), (3, 4),  # Thumb\n",
    "            (0, 5), (5, 6), (6, 7), (7, 8),  # Index finger\n",
    "            (0, 9), (9, 10), (10, 11), (11, 12),  # Middle finger\n",
    "            (0, 13), (13, 14), (14, 15), (15, 16),  # Ring finger\n",
    "            (0, 17), (17, 18), (18, 19), (19, 20)  # Pinky\n",
    "        ]\n",
    "        \n",
    "        for hand in landmarks:\n",
    "            for start, end in connections:\n",
    "                start_x = int(hand[start].x * frame.shape[1])\n",
    "                start_y = int(hand[start].y * frame.shape[0])\n",
    "                end_x = int(hand[end].x * frame.shape[1])\n",
    "                end_y = int(hand[end].y * frame.shape[0])\n",
    "                cv2.line(frame, (start_x, start_y), (end_x, end_y), (255, 0, 0), 2)\n",
    "    \n",
    "        keyposition = draw_keyboard(frame, alpha=0.5)\n",
    "        if recognized_gesture:\n",
    "            if (\"Pointing_Up\" in recognized_gesture):\n",
    "                is_finger_over_key(landmarks[0][INDEX_TIP].x * frame_width, landmarks[0][INDEX_TIP].y * frame_height, keyposition)\n",
    "    \n",
    "        cv2.imshow('Virtual Keyboard', frame)\n",
    "    \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
