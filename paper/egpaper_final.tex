\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{GeReco: A deep learning approach to static and dynamic gesture recognition}

\author{Andrea Auletta\\
{\tt\small andrea.auletta@studenti.unipd.it}
\and
Marco Bernardi\\
{\tt\small marco.bernardi.11@studenti.unipd.it}
\and
Sebastiano Sanson\\
{\tt\small sebastiano.sanson@studenti.unipd.it}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   This research project aims to investigate the current state of the art in gesture recognition, with an emphasis on both static and dynamic gestures. 
   It proposes an inclusive framework designed to recognize a broad spectrum of gestures, ranging from simple static gestures to complex dynamic ones. 
   The proposed framework has significant potential for applications in various domains, including human-computer interaction, robotics, and virtual reality. 
   Specific applications include human sign language recognition, gesture-based interfaces, and gesture-based game control.
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
Gesture recognition is a significant research area in computer vision and machine learning. It can be used in various applications and this is why it is an important topic to study.
The goal of this project is to study, implement and test some approaches to gesture recognition, focusing on both static and dynamic gestures: in the first one we recognize the position of 
the hand while in the second one we recognize the movement. 
We also tried to implement a real time gesture recognition system using a webcam and the developed models.
\section{Related Work}
There are many apporaches to gesture recognition. What we have done is to study some of them and try to implement or to use transfer learning to adapt them to our problem. \\
MENZIONARE ALCUNI APPROCCI TROVATI IN ALTRI PAPER? AD ESEMPIO VISION TRANSFORMERS \\
% PAPER -> https://iris.unimore.it/retrieve/handle/11380/1212263/282584/3DV_2020.pdf
One of the approaches seen is try to fine-tune a \textit{Vision Transformer (ViT)} in order to recognize dynamic gestures. \\
Finally we decided to use the \textit{MediaPipe Hands} \cite{zhang2020mediapipehandsondevicerealtime}, developed through Mediapipe \cite{lugaresi2019mediapipeframeworkbuildingperception} 
to detect static gesture and a custom model to detect dynamic gestures similar to the one proposed by Yaseen et al. \cite{electronics13163233}, that consists on three layers: \textit{Mediapipe}, \textit{InceptionV3} and \textit{LSTM}.

\section{Dataset}
Clearly the dataset is a crucial part of the project and obviously we needed two kind of dataset:
\begin{itemize}
   \item Dataset of \textbf{static gestures}: we used a reduced version of the \textit{HaGRID dataset} \cite{Alexander_2024}. The original dataset is too big for our purposes and for our resources, it contains about 550000 Full-HD images (716GB) divided in 18 classes of gestures. So we decided to use only about 2000 images divided in 6 classes of gestures.
   \item Dataset of \textbf{dynamic gestures}: we used the \textit{Depth camera based dataset of hand gestures} \cite{JEERU2022108659}. In this case there are about 4000 videos of 
   6 different gestures.
\end{itemize}

\section{Method}
\subsection{Static gestures}
Mediapipe provides a good solution for hand detection and tracking. We used it to detect the hand classify the gestures.
The complex part here was to understand how to improve the accuracy of the model and we made it by using transfer learning, data augmentation and hyperparameters tuning.
\subsubsection{Data preprocessing}
For image processing it was important to choose those transformations that did not have the risk of cutting off the hands. We decided to use: \textit{rotation}, \textit{scaling}, \textit{brightness-contrast}, \textit{color conversion}, \textit{blur}, \textit{noise} and \textit{sharpening}. All these transformations were applied with random parameters.
We tried different percentages in addition to the initial dataset and the one which gave us the best results was 25\%.  
\subsubsection{Model}
Mediapipe is a framework for building pipelines to perform infernce over arbitrary sensory data. This has been used to develop a model that can detect the hand and classify the gesture.
The ML pipeline of this is composed by two models working together:
\begin{itemize}
   \item \textbf{Palm detector}: operates on a full input image and locates palms via an oriented hand bounding box. The advantage of detecting the palm instead of the hand 
   is that it is a more rigid structure and it is easier to detect. The feature extraction is done via an encoder-decoder architecture. The idea is to make a recall to the 
   \textit{Feature pyramid networks} \cite{lin2017featurepyramidnetworksobject}:
   \begin{enumerate}
      \item \textit{Bottom-up pathway}: a ConvNet generates feature maps at multiple scales. The deeper layers contain more rich semantic information;
      \item \textit{Top-down pathway}: Upsample the feature maps and merge them with the corresponding feature maps through lateral connections (from the bottom-up pathway).
      This allows to recover the spatial information lost during the downsampling.
   \end{enumerate}
   \item \textbf{Hand landmark model}: operates on the cropped hand bounding box and return the landmarks. 
   The model makes a regression to estimate the coordinates of the 21 hand's landmarks. It will return also the probability of the presence of the hand and 
   the indication if the hand is left or right.
\end{itemize}
We did transfer learning on this model by training only the last layer such that it can learn to classify the gestures we needed.
For the fine-tuning of the model we tuned the \texttt{learning\_rate} and its \texttt{decay}, 
the \texttt{batch\_size}, and the \texttt{layer\_widths} in order to find a trade-off between 
efficiency and accuracy.
We \texttt{shuffled} the dataset before training in order to avoid the model to have some bias.
In order to reduce the overfitting we used the \texttt{dropout} technique to 
let the model learn different representations of the same data by setting a random fraction of 
the units of the network to 0 at each update during training time.
\subsection{Dynamic gestures}
\subsubsection{Data preprocessing}
\label{subsec:datapreprocessing}
The data preprocessing for the dynamic gestures was more complex than the one for the static gestures. As seen in the 
paper \cite{electronics13163233} the idea was to take 10 frames from the video equispaced and to crop the 
\textbf{region of interest (ROI) on the hands} in order to not consider useless information of the background.
After this the image should be resized to a 50x60, but InceptionV3 requires a minimum size 
of 75x75, so we decided to use those dimensions.
We noticed different things that were not considered, starting from the fact that there was no transformation 
to improve the quality of the images. After cropping and resizing the images the quality was 
not good enough to be used for the training. We decided to apply some transformations and the results is the 
one which is presented in the image \ref{fig:transformation_hands}.
We also flipped the images on the vertical axis in a way to recognize the gestures done by both hands.
The idea to consider as ROI only the hands was not the best one for us, because in the dataset, the gestures 
were done with the whole arm and not only with the hands. Furthermore, not in all images mediapipe is able to 
recognize hands and so frames are dropped, but vectors of the same size are required by the \textit{LSTM}. In this way there was a big reduction of the whole dataset.
To solve these problems we tried to train the model by \textbf{cropping the images on the body} of the person 
as in the image \ref{fig:transformation_body}.
We tried to use the \textbf{full image} without cropping but with the full image the model was not able to learn, probably 
because the person was too far from the camera and the background was the biggest part of the image.
The last try was to \textbf{crop directly on the arms} detected by mediapipe, specifying the ROI: considering the \href{https://ai.google.dev/static/mediapipe/images/solutions/pose_landmarks_index.png}{keypoints of the pose detected by mediapipe}, we have selected the ROI covered by the keypoints from 11 to 22. WAITING THE RESULTS\dots
-palo o chiattone

\begin{figure*}[h]
   \centering
   \includegraphics[width=\textwidth]{images/transformation_hands.jpg} 
   \caption{Transformations on ROI of the hands.}
   \label{fig:transformation_hands}
\end{figure*}
\begin{figure*}[h]
   \centering
   \includegraphics[width=\textwidth]{images/transformation_body.jpg} 
   \caption{Transformations on ROI of the body.}
   \label{fig:transformation_body}
\end{figure*}
\subsubsection{Model}
The pipeline is similar to the one explained by Yaseen et al \cite{electronics13163233}, it is composed by three main parts:
\begin{enumerate}
   \item \textbf{MediaPipe}: MediaPipe is used to detect and then crop the ROI from each frame of the video.
   As said in the previous section \ref{subsec:datapreprocessing} we have tried to train the model considering 
   different regions of interest. We tried to crop the hand and the body of the person in the video. 
   \item \textbf{InceptionV3}: we used the InceptionV3 model \cite{szegedy2015rethinkinginceptionarchitecturecomputer} 
   to extract features from the cropped regions of interest.
   The idea of Inception is to have filters with different sizes in the same layer and then concatenate 
   the results. What is trying to solve InceptionV3 with respect to the previous versions are:
   \begin{itemize}
      \item \textit{Reducing the representational bottleneck}: neural networks perform better when convolutions
      didn't alter the dimension of the input drastically, there could be a loss of information;
      \item \textit{Using smart factorization network}
   \end{itemize}  
   These problems are solved by \textit{factorizing filters} (e.g. 5x5 in two of 3x3) and by \textit{expanding 
   the banks of filter in a wider way} instead of having deeper networks. 
   Deeper models means higher reduction and hence loss of information.
   \item \textbf{LSTM}: we used a LSTM layer to classify the gestures. 
   The LSTM is a type of RNN, this means that can take in input a sequence of data. This is useful in our case
   because we are trying to classify a sequence of frames. In the simple RNN there is an internal memory 
   that allows to remember the previous results and the computation of the current input depends on 
   the previous one. The problem with the simple RNN is that it has a problem with the \textit{vanishing gradient}.
   LSTM solves this problem by introducing different components which allows to keep the information 
   for a longer time (forget gate, input gate, output gate).
\end{enumerate}
The only model we trained on here is the LSTM model. The main problem we found during the training 
was the overfitting. We tried to solve this problem by using:
\begin{itemize}
   \item \textit{Dropout}: it works by setting a random fraction of the units of the network to 0 
   at each update during training time. This reduces the overfitting 
   because the network is forced to learn different representations of the same data; ??????????
   \item \textit{L2 regularization}: it works by adding a penalty to the loss function;
   \item \textit{early stopping}: it works by stopping the training when the validation loss 
   has no improvement for a certain number of epochs. We set the number of \textit{epochs} to 
   300 and for our purposes there were too many since it overfitted after fewer epochs.
\end{itemize}
We also found here a trade-off between efficiency and accuracy by tuninig 
the \textit{batch size} and the \textit{learning rate}.





\subsection{Real time gesture recognition}

\section{Experiments}
\subsection{Static gestures}
In the following table \ref{tab:staticGestures} we can see the results of the model 
trained on the static gestures dataset. 
The tests are splitted based on the percentage of the augmented dataset with reference to number of the examples.
\begin{table}[h]
   \begin{center}
   \begin{tabular}{|c|c|c|c|c|}
   \hline
   \textbf{Aug. \%} & \textbf{Val\_loss} & \textbf{Val\_Acc.} & \textbf{Test\_loss} & \textbf{Test\_Acc.}\\
   \hline\hline
   20 & 0.13 & 0.90 & 0.17 & 0.87 \\
   \textbf{25} & 0.16 & 0.89 & \textbf{0.13} & \textbf{0.90} \\
   30 & 0.19 & 0.86 & 0.19 & 0.87 \\
   35 & 0.16 & 0.88 & 0.19 & 0.88 \\
   40 & 0.18 & 0.86 & 0.17 & 0.87 \\
   100 & 0.22 & 0.84 & 0.19 & 0.87 \\ 
   \hline
   \end{tabular}
   \end{center}
   \caption{Results of the model trained on the static gestures dataset.}
   \label{tab:staticGestures}
\end{table} \\
We can say that the best results for the test accuracy are obtained with the 25\% of the 
augmented dataset, intrducing enough variability in the dataset in order to 
improve the robustness of the model, but not too much to make data too different from the original one.
At lower percentages the model could not have seen enough variations in the data to generalize well, and at 
higher percentages the model could have seen too many variations making the learning less effective.
\subsection{Dynamic gestures}
\section{Conclusion and future works}
In this project we focused more on the modification of the dataset to recover frames lost during the preprocessing
instead of finding a way to choose other frames, so this is a possible work to do to improve the model.
We can say that with more resources we could get better results in less time, specially for the dynamic gestures:
\begin{itemize}
   \item \textbf{Camera} for the real time recognition;
   \item \textbf{GPU} for the training of the models;
   \item \textbf{Memory} to store more data and process all the frames of the videos.
\end{itemize}
results....

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
