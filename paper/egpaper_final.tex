\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{GeReco: A deep learning approach to static and dynamic gesture recognition}

\author{Andrea Auletta\\
{\tt\small andrea.auletta@studenti.unipd.it}
\and
Marco Bernardi\\
{\tt\small marco.bernardi.11@studenti.unipd.it}
\and
Sebastiano Sanson\\
{\tt\small sebastiano.sanson@studenti.unipd.it}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   This research project aims to investigate the current state of the art in gesture recognition, with an emphasis on both static and dynamic gestures. 
   It proposes an inclusive framework designed to recognize a broad spectrum of gestures, ranging from simple static gestures to complex dynamic ones. 
   The proposed framework has significant potential for applications in various domains, including human-computer interaction, robotics, and virtual reality. 
   Specific applications include human sign language recognition, gesture-based interfaces, and gesture-based game control.
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
Gesture recognition is a significant research area in computer vision and machine learning. It can be used in various applications and this is why it is an important topic to study.
The goal of this project is to study, implement and test some approaches to gesture recognition, focusing on both static and dynamic gestures: in the first one we recognize the position of 
the hand while in the second one we recognize the movement. 
We also tried to implement a real time gesture recognition system using a webcam and the developed models.
\section{Related Work}
There are many apporaches to gesture recognition. What we have done is to study some of them and try to implement or to use transfer learning to adapt them to our problem.
Finally we decided to use the MediaPipe Hands \cite{zhang2020mediapipehandsondevicerealtime}, developed through Mediapipe \cite{lugaresi2019mediapipeframeworkbuildingperception} 
to detect static gesture and a custom model to detect dynamic gestures similar to that one shown in this paper \cite{electronics13163233}. 
This last model is based on Mediapipe, InceptionV3 and a LSTM layer.

\section{Dataset}
Clearly the dataset is a crucial part of the project and obviously we needed two kind of dataset. For the static gestures 
we used a dataset of images while for the dynamic gestures we used a dataset of videos:
\begin{itemize}
   \item Dataset of \textbf{static gestures}: we used a reduced version of the \textit{HaGRID dataset} \cite{Alexander_2024}. The original size dataset
   is too big for our purposes and for our resources, so we decided to use a reduced version of it. The original dataset contains about 550000 FullHD images (716GB)
   divided in 18 classes of gestures. We decided to use only about 2000 images divided in 6 classes of gestures.
   \item Dataset of \textbf{dynamic gestures}: we used the \textit{Depth camera based dataset of hand gestures} \cite{JEERU2022108659}. In this case there are about 4000 videos of 
   6 different gestures.
\end{itemize}

\section{Method}
\subsection{Static gestures}
Mediapipe provides a good solution for hand detection and tracking. We used it to detect the hand classify the gestures.
The complex part here was to understand how to improve the accuracy of the model and we made it by using transfer learning, data augmentation and hyperparameters tuning.
\subsubsection{Data preprocessing}
For image processing it was important to choose those transformations that did not have the risk of cutting off the hands. We decided to use: rotation, scaling, 
brightness-contrast, color conversion, blur, noise, sharpening. All this transformations were applied with random parameters.
We tried different percentages in addition to the initial dataset and the one which gave us the best results was 25\%.  
\subsubsection{Model}
Mediapipe is a framework for building pipelines to perform infernce over arbitrary sensory data. This has been used to develop a model that can detect the hand and classify the gesture.
The ML pipeline of this is composed by two models working together:
\begin{itemize}
   \item \textbf{Palm detector}: operates on a full input image and locates palms via an oriented hand bounding box. The advantage of detecting the palm instead of the hand 
   is that it is a more rigid structure and it is easier to detect. The feature extraction is done via an encoder-decoder architecture. The idea is to make a recall to the 
   \textit{Feature pyramid networks}:
   \begin{enumerate}
      \item \textit{Bottom-up pathway}: a ConvNet generates feature maps at multiple scales. The deeper layers contain more rich semantic information;
      \item \textit{Top-down pathway}: Upsample the feature maps and merge them with the corresponding feature maps through lateral connections (from the bottom-up pathway).
      This allows to recover the spatial information lost during the downsampling.
   \end{enumerate}
   \item \textbf{Hand landmark model}: operates on the cropped hand bounding box and return the landmarks. 
   The model makes a regression to estimate the coordinates of the 21 hand's landmarks. It will return also the probability of the presence of the hand and 
   the indication if the hand is left or right.
\end{itemize}
We did transfer learning on this model by training only the last layer of the model such that the model can learn to classify the gestures we needed.
For the fine-tuning of the model we modified the sequent hyperparameters, it's reported the best configuration we found:
\begin{itemize}
   \item learning\_rate = 0.0002;
   \item lr\_decay = 0.99 - Learning rate decay to use for gradient descent training;
   \item epohcs = 30;
   \item shuffle = True - True if the dataset is shuffled before training, this is done in order to avoid the model to have some bias;
   \item batch\_size = 128 - trade-off between speed and accuracy. It helped us also in reducing the overfitting;
   \item dropout\_rate = 0.1 - The fraction of the input units to drop, this is used to avoid overfitting;
   \item layer\_widths = [1024, 512, 256, 128, 64] - A list of hidden layer widths for the gesture model. Each element in the list will 
   create a new hidden layer with the specified width.
\end{itemize}
\subsection{Dynamic gestures}
\subsubsection{Data preprocessing}
- 10 frames crop hand
- sharpening
- 10 frames on BODY
\subsubsection{Model}
-Mediapipe
-InceptionV3
-LSTM: accept fixed size
\subsection{Real time gesture recognition}

\section{Experiments}
(20\%, 25\%, 30\%, 35\%, 40\%, 100\%)

\section{Conclusion}



\begin{itemize}
	\item Introduction (10\%): describe the problem you are working on, why it's important, and an overview of your results.
	\item Related Work (10\%): discuss published work or similar apps that relates to your project. How is your approach similar or different from others?
	\item Dataset (15\%): describe the data you are working with for your project. What type of data is it? Where did it come from? How much data are you working with? Did you have to do any preprocessing, filtering, etc., and why?
	\item Method (30\%): discuss your approach for solving the problems that you set up in the introduction. Why is your approach the right thing to do? Did you consider alternative approaches? It may be helpful to include figures, diagrams, or tables to describe your method or compare it with others.
	\item Experiments (30\%): discuss the experiments that you performed. The exact experiments will vary depending on the project, but you might compare with prior work, perform an ablation study to determine the impact of various components of your system, experiment with different hyperparameters or architectural choices. You should include graphs, tables, or other figures to illustrate your experimental results.
	\item Conclusion (5\%): summarize your key results; what have you learned? Suggest ideas for future extensions.
\end{itemize}	






%-------------------------------------------------------------------------
\subsection{Illustrations, graphs, and photographs}

{\small\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]
                   {myfile.eps}
\end{verbatim}
}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
