\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{GeReco: A deep learning approach to static and dynamic gesture recognition}

\author{Andrea Auletta\\
{\tt\small andrea.auletta@studenti.unipd.it}
\and
Marco Bernardi\\
{\tt\small marco.bernardi.11@studenti.unipd.it}
\and
Sebastiano Sanson\\
{\tt\small sebastiano.sanson@studenti.unipd.it}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   This research project aims to investigate the current state of the art in gesture recognition, with an emphasis on both static and dynamic gestures. 
   It proposes an inclusive framework designed to recognize a broad spectrum of gestures, ranging from simple static gestures to complex dynamic ones. 
   The proposed framework has significant potential for applications in various domains, including human-computer interaction, robotics, and virtual reality. 
   Specific applications include human sign language recognition, gesture-based interfaces, and gesture-based game control.
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
Gesture recognition is a significant research area in computer vision and machine learning. It can be used in various applications and this is why it is an important topic to study.
The goal of this project is to study, implement and test some approaches to gesture recognition, focusing on both static and dynamic gestures: in the first one we recognize the position of 
the hand while in the second one we recognize the movement. 
We also tried to implement a real time gesture recognition system using a webcam and the developed models.
\section{Related Work}
There are many apporaches to gesture recognition. What we have done is to study some of them and try to implement or to use transfer learning to adapt them to our problem.
Finally we decided to use the \textit{MediaPipe Hands} \cite{zhang2020mediapipehandsondevicerealtime}, developed through Mediapipe \cite{lugaresi2019mediapipeframeworkbuildingperception} 
to detect static gesture and a custom model to detect dynamic gestures similar to the one proposed by Yaseen et al. \cite{electronics13163233}, that consists on three layers: \textit{Mediapipe}, \textit{InceptionV3} and \textit{LSTM}.

\section{Dataset}
Clearly the dataset is a crucial part of the project and obviously we needed two kind of dataset:
\begin{itemize}
   \item Dataset of \textbf{static gestures}: we used a reduced version of the \textit{HaGRID dataset} \cite{Alexander_2024}. The original dataset is too big for our purposes and for our resources, it contains about 550000 Full-HD images (716GB) divided in 18 classes of gestures. So we decided to use only about 2000 images divided in 6 classes of gestures.
   \item Dataset of \textbf{dynamic gestures}: we used the \textit{Depth camera based dataset of hand gestures} \cite{JEERU2022108659}. In this case there are about 4000 videos of 
   6 different gestures.
\end{itemize}

\section{Method}
\subsection{Static gestures}
Mediapipe provides a good solution for hand detection and tracking. We used it to detect the hand classify the gestures.
The complex part here was to understand how to improve the accuracy of the model and we made it by using transfer learning, data augmentation and hyperparameters tuning.
\subsubsection{Data preprocessing}
For image processing it was important to choose those transformations that did not have the risk of cutting off the hands. We decided to use: rotation, scaling, 
brightness-contrast, color conversion, blur, noise, sharpening. All this transformations were applied with random parameters.
We tried different percentages in addition to the initial dataset and the one which gave us the best results was 25\%.  
\subsubsection{Model}
Mediapipe is a framework for building pipelines to perform infernce over arbitrary sensory data. This has been used to develop a model that can detect the hand and classify the gesture.
The ML pipeline of this is composed by two models working together:
\begin{itemize}
   \item \textbf{Palm detector}: operates on a full input image and locates palms via an oriented hand bounding box. The advantage of detecting the palm instead of the hand 
   is that it is a more rigid structure and it is easier to detect. The feature extraction is done via an encoder-decoder architecture. The idea is to make a recall to the 
   \textit{Feature pyramid networks} \cite{lin2017featurepyramidnetworksobject}:
   \begin{enumerate}
      \item \textit{Bottom-up pathway}: a ConvNet generates feature maps at multiple scales. The deeper layers contain more rich semantic information;
      \item \textit{Top-down pathway}: Upsample the feature maps and merge them with the corresponding feature maps through lateral connections (from the bottom-up pathway).
      This allows to recover the spatial information lost during the downsampling.
   \end{enumerate}
   \item \textbf{Hand landmark model}: operates on the cropped hand bounding box and return the landmarks. 
   The model makes a regression to estimate the coordinates of the 21 hand's landmarks. It will return also the probability of the presence of the hand and 
   the indication if the hand is left or right.
\end{itemize}
We did transfer learning on this model by training only the last layer such that it can learn to classify the gestures we needed.
For the fine-tuning of the model we tuned the \texttt{learning\_rate} and its \texttt{decay}, 
the \texttt{batch\_size}, and the \texttt{layer\_widths} in order to find a trade-off between 
efficiency and accuracy.
We \texttt{shuffled} the dataset before training in order to avoid the model to have some bias.
In order to reduce the overfitting we used the \texttt{dropout} technique to 
let the model learn different representations of the same data by setting a random fraction of 
the units of the network to 0 at each update during training time.
\subsection{Dynamic gestures}
\subsubsection{Data preprocessing}
\label{subsec:datapreprocessing}
- 10 frames crop hand
- 10 frames on BODY
- transformations to improve the quality of the images
- for the fixed size lstm
\subsubsection{Model}
The pipeline is similar to the one explained by Yaseen et al \cite{electronics13163233}, it is composed by three main parts:
\begin{enumerate}
   \item \textbf{MediaPipe}: MediaPipe is used to crop the region of interest from each frame of the video.
   As said in the previous section \ref{subsec:datapreprocessing} we have tried to train the model considering 
   different regions of interest. We tried to crop the hand and the body of the person in the video. 
   \item \textbf{InceptionV3}: we used the InceptionV3 model \cite{szegedy2015rethinkinginceptionarchitecturecomputer} 
   to extract features from the cropped regions of interest.
   The idea of Inception is to have filters with different sizes in the same layer and then concatenate 
   the results. What is trying to solve InceptionV3 with respect to the previous versions are:
   \begin{itemize}
      \item \textit{Reducing the representational bottleneck}: neural networks perform better when convolutions
      didn't alter the dimension of the input drastically, there could be a loss of information;
      \item \textit{Using smart factorization network}
   \end{itemize}  
   These problems are solved by \textit{factorizing filters} (e.g. 5x5 in two of 3x3) and by \textit{expanding 
   the banks of filter in a wider way} instead of having deeper networks. 
   Deeper models means higher reduction and hence loss of information.
   \item \textbf{LSTM}: we used a LSTM layer to classify the gestures. 
   The LSTM is a type of RNN, this means that can take in input a sequence of data. This is useful in our case
   because we are trying to classify a sequence of frames. In the simple RNN there is an internal memory 
   that allows to remember the previous results and the computation of the current input depends on 
   the previous one. The problem with the simple RNN is that it has a problem with the \textit{vanishing gradient}.
   LSTM solves this problem by introducing different components which allows to keep the information 
   for a longer time (forget gate, input gate, output gate).
\end{enumerate}
The only model we trained on here is the LSTM model. The main problem we found during the training 
was the overfitting. We tried to solve this problem by using:
\begin{itemize}
   \item \textit{Dropout}: it works by setting a random fraction of the units of the network to 0 
   at each update during training time. This reduces the overfitting 
   because the network is forced to learn different representations of the same data;
   \item \textit{L2 regularization}: it works by adding a penalty to the loss function;
   \item \textit{early stopping}: it works by stopping the training when the validation loss 
   has no improvement for a certain number of epochs. We set the number of \textit{epochs} to 
   300 and for our purposes there were too many since it overfitted after fewer epochs.
\end{itemize}
We also found here a trade-off between efficiency and accuracy by tuninig 
the \textit{batch size} and the \textit{learning rate}.





\subsection{Real time gesture recognition}

\section{Experiments}
\subsection{Static gestures}
In the following table \ref{tab:staticGestures} we can see the results of the model 
trained on the static gestures dataset. 
The tests are splitted based on the percentage of the augmented dataset with reference to number of the examples.
\begin{table}[h]
   \begin{center}
   \begin{tabular}{|c|c|c|c|c|}
   \hline
   \textbf{Aug. \%} & \textbf{Val\_loss} & \textbf{Val\_Acc.} & \textbf{Test\_loss} & \textbf{Test\_Acc.}\\
   \hline\hline
   20 & 0.13 & 0.90 & 0.17 & 0.87 \\
   \textbf{25} & 0.16 & 0.89 & \textbf{0.13} & \textbf{0.90} \\
   30 & 0.19 & 0.86 & 0.19 & 0.87 \\
   35 & 0.16 & 0.88 & 0.19 & 0.88 \\
   40 & 0.18 & 0.86 & 0.17 & 0.87 \\
   100 & 0.22 & 0.84 & 0.19 & 0.87 \\ 
   \hline
   \end{tabular}
   \end{center}
   \caption{Results of the model trained on the static gestures dataset.}
   \label{tab:staticGestures}
\end{table} \\
We can say that the best results for the test accuracy are obtained with the 25\% of the 
augmented dataset. An augmentation of the 25\% intrduces enough variability in the dataset in order to 
improve the robustness of the model, but not too much to make data too different from the original one.
At lower percentages the model could not have seen enough variations in the data to generalize well, and at 
higher percentages the model could have seen too many variations making the learning less effective.
\subsection{Dynamic gestures}
\section{Conclusion}






%-------------------------------------------------------------------------
\subsection{Illustrations, graphs, and photographs}

{\small\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]
                   {myfile.eps}
\end{verbatim}
}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
