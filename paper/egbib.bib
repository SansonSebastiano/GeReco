@misc{zhang2020mediapipehandsondevicerealtime,
      title={MediaPipe Hands: On-device Real-time Hand Tracking}, 
      author={Fan Zhang and Valentin Bazarevsky and Andrey Vakunov and Andrei Tkachenka and George Sung and Chuo-Ling Chang and Matthias Grundmann},
      year={2020},
      eprint={2006.10214},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2006.10214}, 
}

@misc{lugaresi2019mediapipeframeworkbuildingperception,
      title={MediaPipe: A Framework for Building Perception Pipelines}, 
      author={Camillo Lugaresi and Jiuqiang Tang and Hadon Nash and Chris McClanahan and Esha Uboweja and Michael Hays and Fan Zhang and Chuo-Ling Chang and Ming Guang Yong and Juhyun Lee and Wan-Teh Chang and Wei Hua and Manfred Georg and Matthias Grundmann},
      year={2019},
      eprint={1906.08172},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/1906.08172}, 
}

@Article{electronics13163233,
AUTHOR = {Yaseen and Kwon, Oh-Jin and Kim, Jaeho and Jamil, Sonain and Lee, Jinhee and Ullah, Faiz},
TITLE = {Next-Gen Dynamic Hand Gesture Recognition: MediaPipe, Inception-v3 and LSTM-Based Enhanced Deep Learning Model},
JOURNAL = {Electronics},
VOLUME = {13},
YEAR = {2024},
NUMBER = {16},
ARTICLE-NUMBER = {3233},
URL = {https://www.mdpi.com/2079-9292/13/16/3233},
ISSN = {2079-9292},
ABSTRACT = {Gesture recognition is crucial in computer vision-based applications, such as drone control, gaming, virtual and augmented reality (VR/AR), and security, especially in human–computer interaction (HCI)-based systems. There are two types of gesture recognition systems, i.e., static and dynamic. However, our focus in this paper is on dynamic gesture recognition. In dynamic hand gesture recognition systems, the sequences of frames, i.e., temporal data, pose significant processing challenges and reduce efficiency compared to static gestures. These data become multi-dimensional compared to static images because spatial and temporal data are being processed, which demands complex deep learning (DL) models with increased computational costs. This article presents a novel triple-layer algorithm that efficiently reduces the 3D feature map into 1D row vectors and enhances the overall performance. First, we process the individual images in a given sequence using the MediaPipe framework and extract the regions of interest (ROI). The processed cropped image is then passed to the Inception-v3 for the 2D feature extractor. Finally, a long short-term memory (LSTM) network is used as a temporal feature extractor and classifier. Our proposed method achieves an average accuracy of more than 89.7%. The experimental results also show that the proposed framework outperforms existing state-of-the-art methods.},
DOI = {10.3390/electronics13163233}
}

@article{JEERU2022108659,
    title = {Depth camera based dataset of hand gestures},
    journal = {Data in Brief},
    volume = {45},
    pages = {108659},
    year = {2022},
    issn = {2352-3409},
    doi = {https://doi.org/10.1016/j.dib.2022.108659},
    url = {https://www.sciencedirect.com/science/article/pii/S2352340922008642},
    author = {Sindhusha Jeeru and Arun Kumar Sivapuram and David González León and Jade Gröli and Sreenivasa Reddy Yeduri and Linga Reddy Cenkeramaddi},
    keywords = {Video hand gestures, RGB image, Depth image, RGB-D Camera, Machine learning},
    abstract = {The dataset contains RGB and depth version video frames of various hand movements captured with the Intel RealSense Depth Camera D435. The camera has two channels for collecting both RGB and depth frames at the same time. A large dataset is created for accurate classification of hand gestures under complex backgrounds. The dataset is made up of 29718 frames from RGB and depth versions corresponding to various hand gestures from different people collected at different time instances with complex backgrounds. Hand movements corresponding to scroll-right, scroll-left, scroll-up, scroll-down, zoom-in, and zoom-out are included in the data. Each sequence has data of 40 frames, and there is a total of 662 sequences corresponding to each gesture in the dataset. To capture all the variations in the dataset, the hand is oriented in various ways while capturing.}
    }

@inproceedings{Alexander_2024,
   title={HaGRID – HAnd Gesture Recognition Image Dataset},
   url={http://dx.doi.org/10.1109/WACV57701.2024.00451},
   DOI={10.1109/wacv57701.2024.00451},
   booktitle={2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
   publisher={IEEE},
   author={Alexander, Kapitanov and Karina, Kvanchiani and Alexander, Nagaev and Roman, Kraynov and Andrei, Makhliarchuk},
   year={2024},
   month=jan }

@misc{szegedy2015rethinkinginceptionarchitecturecomputer,
      title={Rethinking the Inception Architecture for Computer Vision}, 
      author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
      year={2015},
      eprint={1512.00567},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.00567}, 
}

@misc{lin2017featurepyramidnetworksobject,
      title={Feature Pyramid Networks for Object Detection}, 
      author={Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
      year={2017},
      eprint={1612.03144},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1612.03144}, 
}