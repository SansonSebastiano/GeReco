{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "KERAS_MODEL_PATH = 'C:\\\\Users\\\\Marco\\\\Downloads\\\\Telegram Desktop\\\\model.keras'\n",
    "TFLITE_MODEL_PATH = 'model.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Label Mapping ---\n",
    "label_mapping = {\n",
    "    'scroll_down': 0,\n",
    "    'scroll_left': 1,\n",
    "    'scroll_right': 2,\n",
    "    'scroll_up': 3,\n",
    "    'zoom_in': 4,\n",
    "    'zoom_out': 5\n",
    "}\n",
    "# Create a reverse mapping for display purposes: index -> label\n",
    "index_to_label = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# --- Load Models ---\n",
    "# Load InceptionV3 as a feature extractor (for converting frames to 2048-dim feature vectors)\n",
    "feature_extractor = keras.applications.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    pooling='avg'  # Global average pooling to get a 2048-dim feature vector\n",
    ")\n",
    "# Expected input size for InceptionV3\n",
    "INCEPTION_SIZE = (75, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFLITE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gesture recognition model as a TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output tensor details from the TFLite model\n",
    "input_details = interpreter.get_input_details()    # Expected shape: [1, 10, 2048]\n",
    "output_details = interpreter.get_output_details()  # Expected shape: [1, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested FPS: 5\n",
      "Actual FPS: 5.0\n",
      "Recording gesture...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Captured features shape: (10, 2048)\n",
      "Predicted probabilities:\n",
      "scroll_down: 0.0156\n",
      "scroll_left: 0.0110\n",
      "scroll_right: 0.0008\n",
      "scroll_up: 0.0005\n",
      "zoom_in: 0.0019\n",
      "zoom_out: 0.9702\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Initialize Webcam Capture ---\n",
    "cap = cv2.VideoCapture(0)  # 0 is typically the default webcam\n",
    "\n",
    "desired_fps = 5\n",
    "cap.set(cv2.CAP_PROP_FPS, desired_fps)\n",
    "\n",
    "# Read back the FPS value to check if it was set correctly\n",
    "actual_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(\"Requested FPS:\", desired_fps)\n",
    "print(\"Actual FPS:\", actual_fps)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open webcam.\")\n",
    "\n",
    "# --- Countdown Before Recording ---\n",
    "countdown_time = 3  # seconds\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Calculate remaining time for the countdown\n",
    "    elapsed = time.time() - start_time\n",
    "    remaining = int(countdown_time - elapsed) + 1  # +1 to make the countdown feel natural\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "\n",
    "    # Overlay the countdown text on the frame\n",
    "    cv2.putText(frame, f\"Recording starts in {remaining}...\", (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "print(\"Recording gesture...\")\n",
    "\n",
    "# --- Capture Frames for Gesture ---\n",
    "frames_captured = 0\n",
    "features_list = []\n",
    "\n",
    "while frames_captured < 10:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        continue\n",
    "\n",
    "    # Optionally display the frame while capturing\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Save Each frame as png\n",
    "    cv2.imwrite(f\"frame_{frames_captured}.png\", frame)\n",
    "\n",
    "    # --- Preprocess Each Frame ---\n",
    "    # Resize frame to the expected input size for InceptionV3\n",
    "    frame_resized = cv2.resize(frame, INCEPTION_SIZE)\n",
    "    # Convert BGR (default in OpenCV) to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    # Convert to float32 and preprocess for InceptionV3\n",
    "    frame_rgb = frame_rgb.astype('float32')\n",
    "    preprocessed_frame = keras.applications.inception_v3.preprocess_input(frame_rgb)\n",
    "    # Expand dims to create a batch dimension (1, 299, 299, 3)\n",
    "    preprocessed_frame = np.expand_dims(preprocessed_frame, axis=0)\n",
    "\n",
    "    # --- Extract Features Using InceptionV3 ---\n",
    "    features = feature_extractor.predict(preprocessed_frame)\n",
    "    # features has shape (1, 2048); extract the feature vector\n",
    "    features = features[0]\n",
    "    features_list.append(features)\n",
    "\n",
    "    frames_captured += 1\n",
    "\n",
    "# Release the webcam and close any OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert the list of feature vectors into a numpy array with shape (10, 2048)\n",
    "features_array = np.array(features_list, dtype='float32')\n",
    "print(\"Captured features shape:\", features_array.shape)\n",
    "\n",
    "# --- Prepare Features for TFLite LSTM Model ---\n",
    "# The LSTM expects a batch dimension, so reshape to (1, 10, 2048)\n",
    "input_for_lstm = np.expand_dims(features_array, axis=0)\n",
    "\n",
    "# --- Perform Inference with the TFLite Model ---\n",
    "# Ensure the input data matches the expected type\n",
    "input_data = input_for_lstm.astype(input_details[0]['dtype'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "# Retrieve the output from the model\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "# output_data is of shape (1, num_classes); extract probabilities from the first (and only) sample\n",
    "predicted_probabilities = output_data[0]\n",
    "\n",
    "# --- Print the Predicted Probabilities for Each Gesture ---\n",
    "print(\"Predicted probabilities:\")\n",
    "for i, prob in enumerate(predicted_probabilities):\n",
    "    print(f\"{index_to_label[i]}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Load your pre-trained models ---\n",
    "\n",
    "# Load the LSTM model (replace 'model.keras' with your model filename)\n",
    "lstm_model = keras.models.load_model(KERAS_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording gesture...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Captured features shape: (10, 2048)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "Predicted probabilities:\n",
      "scroll_down: 0.1233\n",
      "scroll_left: 0.1292\n",
      "scroll_right: 0.0419\n",
      "scroll_up: 0.0077\n",
      "zoom_in: 0.0018\n",
      "zoom_out: 0.6961\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Initialize webcam capture ---\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # 0 is typically the default webcam\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open webcam.\")\n",
    "\n",
    "# --- Countdown Before Recording ---\n",
    "countdown_time = 3  # seconds\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Calculate remaining time for the countdown\n",
    "    elapsed = time.time() - start_time\n",
    "    remaining = int(countdown_time - elapsed) + 1  # +1 for a more natural countdown display\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "\n",
    "    # Overlay the countdown text on the frame\n",
    "    cv2.putText(frame, f\"Recording starts in {remaining}...\", (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "print(\"Recording gesture...\")\n",
    "\n",
    "# --- Capture Frames for Gesture ---\n",
    "frames_captured = 0\n",
    "features_list = []\n",
    "\n",
    "while frames_captured < 10:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        continue\n",
    "\n",
    "    # Optionally display the frame while capturing\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # --- Preprocess Each Frame ---\n",
    "    # Resize to InceptionV3 expected size\n",
    "    frame_resized = cv2.resize(frame, INCEPTION_SIZE)\n",
    "    # Convert BGR (OpenCV default) to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    # Convert to float32 and preprocess using InceptionV3 preprocessor\n",
    "    frame_rgb = frame_rgb.astype('float32')\n",
    "    preprocessed_frame = keras.applications.inception_v3.preprocess_input(frame_rgb)\n",
    "    # Expand dims to create batch dimension: (1, 299, 299, 3)\n",
    "    preprocessed_frame = np.expand_dims(preprocessed_frame, axis=0)\n",
    "\n",
    "    # --- Extract Features Using InceptionV3 ---\n",
    "    features = feature_extractor.predict(preprocessed_frame)\n",
    "    # features will have shape (1, 2048); extract the vector\n",
    "    features = features[0]\n",
    "    features_list.append(features)\n",
    "\n",
    "    frames_captured += 1\n",
    "\n",
    "# Release the webcam and close any OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert list of feature vectors into a numpy array with shape (10, 2048)\n",
    "features_array = np.array(features_list, dtype='float32')\n",
    "print(\"Captured features shape:\", features_array.shape)\n",
    "\n",
    "# --- Prepare Features for LSTM Model ---\n",
    "# The LSTM expects a batch dimension, so reshape to (1, 10, 2048)\n",
    "input_for_lstm = np.expand_dims(features_array, axis=0)\n",
    "\n",
    "# --- Perform Inference with the LSTM Model ---\n",
    "prediction = lstm_model.predict(input_for_lstm)\n",
    "# prediction is an array of shape (1, num_classes); extract the probabilities\n",
    "predicted_probabilities = prediction[0]\n",
    "\n",
    "# Print the predicted probabilities for each gesture\n",
    "print(\"Predicted probabilities:\")\n",
    "for i, prob in enumerate(predicted_probabilities):\n",
    "    print(f\"{index_to_label[i]}: {prob:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
