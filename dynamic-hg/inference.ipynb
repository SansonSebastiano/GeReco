{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "KERAS_MODEL_PATH = 'C:\\\\Users\\\\Marco\\\\Downloads\\\\Telegram Desktop\\\\model.keras'\n",
    "TFLITE_MODEL_PATH = '/Users/sebastianosanson/Development/VCS_Project/dynamic-hgr/transformed_pose_saved_model.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pose landmarks\n",
    "\n",
    "MARGIN = 100  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54)  # vibrant green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Label Mapping ---\n",
    "label_mapping = {\n",
    "    'scroll_down': 0,\n",
    "    'scroll_left': 1,\n",
    "    'scroll_right': 2,\n",
    "    'scroll_up': 3,\n",
    "    'zoom_in': 4,\n",
    "    'zoom_out': 5\n",
    "}\n",
    "# Create a reverse mapping for display purposes: index -> label\n",
    "index_to_label = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# --- Load Models ---\n",
    "# Load InceptionV3 as a feature extractor (for converting frames to 2048-dim feature vectors)\n",
    "feature_extractor = keras.applications.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    pooling='avg'  # Global average pooling to get a 2048-dim feature vector\n",
    ")\n",
    "# Expected input size for InceptionV3\n",
    "INCEPTION_SIZE = (75, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pose_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "  x_coordinates = []\n",
    "  y_coordinates = []\n",
    "\n",
    "  # Check if any pose landmarks were detected\n",
    "  if pose_landmarks_list:\n",
    "    # Loop through the detected poses to visualize.\n",
    "    for idx in range(len(pose_landmarks_list)):\n",
    "      pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "      # Draw the pose landmarks.\n",
    "      pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "      pose_landmarks_proto.landmark.extend([\n",
    "        landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "      ])\n",
    "      solutions.drawing_utils.draw_landmarks(\n",
    "        annotated_image,\n",
    "        pose_landmarks_proto,\n",
    "        solutions.pose.POSE_CONNECTIONS,\n",
    "        solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "      # Get the coordinates of all the keypoints\n",
    "      height, width, _ = annotated_image.shape\n",
    "      x_coordinates += [int(landmark.x * width) for landmark in pose_landmarks]\n",
    "      y_coordinates += [int(landmark.y * height) for landmark in pose_landmarks]\n",
    "\n",
    "    return annotated_image, x_coordinates, y_coordinates\n",
    "  else:\n",
    "    # Return default values or handle the case when no landmarks are found\n",
    "    return rgb_image, [], []  # Return original image and empty lists for x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(rgb_image, x_coordinates, y_coordinates):\n",
    "\n",
    "    height, width, _ = rgb_image.shape\n",
    "\n",
    "    x_min = max(min(x_coordinates) - MARGIN, 0)\n",
    "    x_max = min(max(x_coordinates) + MARGIN, width)\n",
    "    y_min = max(min(y_coordinates) - MARGIN, 0)\n",
    "    y_max = min(max(y_coordinates) + MARGIN, height)\n",
    "\n",
    "    return rgb_image[y_min:y_max, x_min:x_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropping_on_pose(cv2image):\n",
    "    # Convert the image from BGR (OpenCV default) to RGB.\n",
    "    image_rgb = cv2.cvtColor(cv2image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create a Mediapipe Image object with the appropriate format.\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2image)\n",
    "    \n",
    "    # Set up the pose landmarker options.\n",
    "    base_options = python.BaseOptions(model_asset_path='/Users/sebastianosanson/Development/VCS_Project/task-model/pose_landmarker_heavy.task')\n",
    "    options = vision.PoseLandmarkerOptions(\n",
    "        base_options=base_options, \n",
    "        output_segmentation_masks=True\n",
    "    )\n",
    "    detector = vision.PoseLandmarker.create_from_options(options)\n",
    "    \n",
    "    # Detect the pose landmarks.\n",
    "    detection_result = detector.detect(mp_image)\n",
    "    \n",
    "    # Draw landmarks (assuming your function supports Mediapipe's Image objects).\n",
    "    annotated_image, x, y = draw_pose_landmarks_on_image(mp_image.numpy_view(), detection_result)\n",
    "    \n",
    "    # Check if the necessary coordinates were found.\n",
    "    if not x or not y:\n",
    "        return cv2image\n",
    "    else:\n",
    "        # Crop the image using your custom crop function.\n",
    "        cropped_image = crop(mp_image.numpy_view(), x, y)\n",
    "        return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFLITE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 38 nodes with 2 partitions.\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the gesture recognition model as a TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output tensor details from the TFLite model\n",
    "input_details = interpreter.get_input_details()    # Expected shape: [1, 10, 2048]\n",
    "output_details = interpreter.get_output_details()  # Expected shape: [1, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 19:30:38.163 python[32943:414237] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested FPS: 5\n",
      "Actual FPS: 30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 19:30:40.065 python[32943:414237] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-07 19:30:40.065 python[32943:414237] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording gesture...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738953046.994218  414237 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1738953047.156856  571235 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953047.256471  571234 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953047.361716  571235 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Extracted features shape: (1, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738953048.720818  414237 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738953048.880343  571506 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953048.974605  571506 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "Extracted features shape: (1, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738953049.628889  414237 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738953049.825006  571640 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953049.927353  571639 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738953050.230189  414237 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738953050.384927  571745 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953050.470552  571745 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738953050.696778  414237 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738953050.953276  571822 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953051.011499  571822 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738953051.226348  414237 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738953051.387514  571920 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953051.452963  571920 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738953051.664046  414237 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738953051.803390  572010 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953051.851074  572013 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738953052.032082  414237 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738953052.218618  572097 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953052.270853  572097 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738953052.461853  414237 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1738953052.591094  572201 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953052.642736  572201 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1738953052.842317  414237 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (1, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1738953052.986639  572281 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738953053.032122  572283 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Captured features shape: (10, 2048)\n",
      "Predicted probabilities:\n",
      "scroll_down: 0.2373\n",
      "scroll_left: 0.1400\n",
      "scroll_right: 0.0624\n",
      "scroll_up: 0.0094\n",
      "zoom_in: 0.2234\n",
      "zoom_out: 0.3275\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap  = cv2.VideoCapture(1)  # 0 is typically the default webcam\n",
    "\n",
    "cap_width = 160\n",
    "cap_height = 120\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, cap_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, cap_height)\n",
    "\n",
    "desired_fps = 5\n",
    "cap.set(cv2.CAP_PROP_FPS, desired_fps)\n",
    "\n",
    "# Read back the FPS value to check if it was set correctly\n",
    "actual_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(\"Requested FPS:\", desired_fps)\n",
    "print(\"Actual FPS:\", actual_fps)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open webcam.\")\n",
    "\n",
    "# --- Countdown Before Recording ---\n",
    "countdown_time = 5  # seconds\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # Mirror the frame\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    remaining = int(countdown_time - elapsed) + 1  # +1 to make the countdown feel natural\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "\n",
    "    cv2.putText(frame, f\"Recording starts in {remaining}...\", (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "print(\"Recording gesture...\")\n",
    "# CV2 Print on frame Recording gesture\n",
    "\n",
    "cv2.putText(frame, \"Recording gesture...\", (50, 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "# --- Capture Frames for Gesture ---\n",
    "frames_captured = 0\n",
    "captured_frames = []  # list to store the raw frames\n",
    "\n",
    "while frames_captured < 40:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        continue\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # Mirror the frame\n",
    "\n",
    "    # Optionally display the frame while capturing\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Append frame to our list\n",
    "    captured_frames.append(frame.copy())\n",
    "    # Optionally save each frame as PNG\n",
    "    cv2.imwrite(f\"/Users/sebastianosanson/Development/VCS_Project/captured_frames/frame_{frames_captured}.png\", frame)\n",
    "\n",
    "    frames_captured += 1\n",
    "\n",
    "# Release the webcam and close any OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "selected_frames = captured_frames[::4]  # Take every 4th frame\n",
    "\n",
    "# --- Process All Captured Frames ---\n",
    "features_list = []\n",
    "\n",
    "for idx, frame in enumerate(selected_frames):\n",
    "    # --- Cropping on Pose ---\n",
    "    cropped_image = cropping_on_pose(frame)\n",
    "    if cropped_image is None:\n",
    "        print(f\"Cropping failed on frame {idx}. Skipping this frame.\")\n",
    "        continue\n",
    "\n",
    "    cv2.imwrite(f\"/Users/sebastianosanson/Development/VCS_Project/cropped_frames/frame_{idx}.png\", cropped_image)\n",
    "\n",
    "    # --- Preprocess Each Frame ---\n",
    "    # Resize frame to the expected input size for InceptionV3\n",
    "    frame_resized = cv2.resize(cropped_image, INCEPTION_SIZE)\n",
    "\n",
    "    cv2.imwrite(f\"/Users/sebastianosanson/Development/VCS_Project/resized_frames/frame_{idx}.png\", frame_resized)\n",
    "    # Convert BGR (default in OpenCV) to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    # Convert to tensor and add batch dimension\n",
    "    image_tensor = tf.convert_to_tensor(frame_rgb, dtype=tf.float32)\n",
    "    print(f\"Preprocessed frame shape: {image_tensor.shape}\")\n",
    "\n",
    "    preprocessed_frame = np.expand_dims(image_tensor, axis=0)\n",
    "\n",
    "    # --- Extract Features Using InceptionV3 ---\n",
    "    features = feature_extractor.predict(preprocessed_frame)\n",
    "    # Assuming features shape is (1, 2048), extract the feature vector\n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "    features_list.append(features[0])\n",
    "\n",
    "if not features_list:\n",
    "    raise ValueError(\"No valid frames were processed.\")\n",
    "\n",
    "# Convert the list of feature vectors into a numpy array with shape (N, 2048)\n",
    "features_array = np.array(features_list, dtype='float32')\n",
    "print(\"Captured features shape:\", features_array.shape)\n",
    "\n",
    "# --- Prepare Features for TFLite LSTM Model ---\n",
    "# The LSTM expects a batch dimension, so reshape to (1, N, 2048)\n",
    "input_for_lstm = np.expand_dims(features_array, axis=0)\n",
    "\n",
    "# --- Perform Inference with the TFLite Model ---\n",
    "input_data = input_for_lstm.astype(input_details[0]['dtype'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "predicted_probabilities = output_data[0]\n",
    "\n",
    "# --- Print the Predicted Probabilities for Each Gesture ---\n",
    "print(\"Predicted probabilities:\")\n",
    "for i, prob in enumerate(predicted_probabilities):\n",
    "    print(f\"{index_to_label[i]}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Load your pre-trained models ---\n",
    "\n",
    "# Load the LSTM model (replace 'model.keras' with your model filename)\n",
    "lstm_model = keras.models.load_model(KERAS_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording gesture...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Captured features shape: (10, 2048)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "Predicted probabilities:\n",
      "scroll_down: 0.1233\n",
      "scroll_left: 0.1292\n",
      "scroll_right: 0.0419\n",
      "scroll_up: 0.0077\n",
      "zoom_in: 0.0018\n",
      "zoom_out: 0.6961\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Initialize webcam capture ---\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # 0 is typically the default webcam\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open webcam.\")\n",
    "\n",
    "# --- Countdown Before Recording ---\n",
    "countdown_time = 3  # seconds\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Calculate remaining time for the countdown\n",
    "    elapsed = time.time() - start_time\n",
    "    remaining = int(countdown_time - elapsed) + 1  # +1 for a more natural countdown display\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "\n",
    "    # Overlay the countdown text on the frame\n",
    "    cv2.putText(frame, f\"Recording starts in {remaining}...\", (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "print(\"Recording gesture...\")\n",
    "\n",
    "# --- Capture Frames for Gesture ---\n",
    "frames_captured = 0\n",
    "features_list = []\n",
    "\n",
    "while frames_captured < 10:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        continue\n",
    "\n",
    "    # Optionally display the frame while capturing\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # --- Preprocess Each Frame ---\n",
    "    # Resize to InceptionV3 expected size\n",
    "    frame_resized = cv2.resize(frame, INCEPTION_SIZE)\n",
    "    # Convert BGR (OpenCV default) to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    # Convert to float32 and preprocess using InceptionV3 preprocessor\n",
    "    frame_rgb = frame_rgb.astype('float32')\n",
    "    preprocessed_frame = keras.applications.inception_v3.preprocess_input(frame_rgb)\n",
    "    # Expand dims to create batch dimension: (1, 299, 299, 3)\n",
    "    preprocessed_frame = np.expand_dims(preprocessed_frame, axis=0)\n",
    "\n",
    "    # --- Extract Features Using InceptionV3 ---\n",
    "    features = feature_extractor.predict(preprocessed_frame)\n",
    "    # features will have shape (1, 2048); extract the vector\n",
    "    features = features[0]\n",
    "    features_list.append(features)\n",
    "\n",
    "    frames_captured += 1\n",
    "\n",
    "# Release the webcam and close any OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert list of feature vectors into a numpy array with shape (10, 2048)\n",
    "features_array = np.array(features_list, dtype='float32')\n",
    "print(\"Captured features shape:\", features_array.shape)\n",
    "\n",
    "# --- Prepare Features for LSTM Model ---\n",
    "# The LSTM expects a batch dimension, so reshape to (1, 10, 2048)\n",
    "input_for_lstm = np.expand_dims(features_array, axis=0)\n",
    "\n",
    "# --- Perform Inference with the LSTM Model ---\n",
    "prediction = lstm_model.predict(input_for_lstm)\n",
    "# prediction is an array of shape (1, num_classes); extract the probabilities\n",
    "predicted_probabilities = prediction[0]\n",
    "\n",
    "# Print the predicted probabilities for each gesture\n",
    "print(\"Predicted probabilities:\")\n",
    "for i, prob in enumerate(predicted_probabilities):\n",
    "    print(f\"{index_to_label[i]}: {prob:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
