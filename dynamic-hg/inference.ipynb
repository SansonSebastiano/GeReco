{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Hand Gestures Recognition Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO_ID = \"Seba213/rgb-dhgr-dataset\"\n",
    "DYNAMIC_MODEL = \"saved_model.tflite\"\n",
    "model_path = hf_hub_download(repo_id=REPO_ID, filename=DYNAMIC_MODEL, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with your own path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the saved model\n",
    "TFLITE_MODEL_PATH = model_path\n",
    "# Path to the folder containing the captured frames\n",
    "CAPTURED_FRAMES = \".../captured_frames\"\n",
    "# Path to the folder containing the cropped frames\n",
    "CROPPED_FRAMES = \".../cropped_frames\"\n",
    "# Path to the folder containing the resized frames\n",
    "RESIZED_FRAMES = \".../resized_frames\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the classes labels map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'scroll_down': 0,\n",
    "    'scroll_left': 1,\n",
    "    'scroll_right': 2,\n",
    "    'scroll_up': 3,\n",
    "    'zoom_in': 4,\n",
    "    'zoom_out': 5\n",
    "}\n",
    "\n",
    "index_to_label = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "feature_extractor = keras.applications.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    pooling='avg'  \n",
    ")\n",
    "\n",
    "INCEPTION_SIZE = (75, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_sides(image, pixels=20):\n",
    "    '''\n",
    "    Crop the input image by a specified number of pixels on both the left and right sides.\n",
    "\n",
    "    Args:\n",
    "        image (np.array): The input image.\n",
    "        pixels (int): The number of pixels to crop from each side.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The cropped image.\n",
    "    '''\n",
    "    height, width, _ = image.shape\n",
    "    cropped_image = image[:, pixels:width-pixels]\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gesture recognition model as a TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output tensor details from the TFLite model\n",
    "input_details = interpreter.get_input_details()    # Expected shape: [1, 10, 2048]\n",
    "output_details = interpreter.get_output_details()  # Expected shape: [1, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested FPS: 5\n",
      "Actual FPS: 30.0\n",
      "Recording gesture...\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Preprocessed frame shape: (75, 75, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Extracted features shape: (1, 2048)\n",
      "Captured features shape: (10, 2048)\n",
      "Predicted probabilities:\n",
      "scroll_down: 0.0000\n",
      "scroll_left: 0.0000\n",
      "scroll_right: 0.9994\n",
      "scroll_up: 0.0006\n",
      "zoom_in: 0.0000\n",
      "zoom_out: 0.0000\n"
     ]
    }
   ],
   "source": [
    "cap  = cv2.VideoCapture(0) \n",
    "# Setting the size of the frame\n",
    "cap_width = 160\n",
    "cap_height = 120\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, cap_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, cap_height)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open webcam.\")\n",
    "\n",
    "countdown_time = 5  \n",
    "start_time = time.time()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # Mirror the frame\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    remaining = int(countdown_time - elapsed) + 1  \n",
    "    if remaining <= 0:\n",
    "        break\n",
    "\n",
    "    cv2.putText(frame, f\"Recording starts in {remaining}...\", (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "print(\"Recording gesture...\")\n",
    "\n",
    "cv2.putText(frame, \"Recording gesture...\", (50, 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "frames_captured = 0\n",
    "captured_frames = []  # List to store the raw frames\n",
    "\n",
    "while frames_captured < 40:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        continue\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # Mirror the frame\n",
    "\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    captured_frames.append(frame.copy())\n",
    "    cv2.imwrite(f\"{CAPTURED_FRAMES}/frame_{frames_captured}.png\", frame)\n",
    "\n",
    "    frames_captured += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "selected_frames = captured_frames[::4]  # Take every 4th frame\n",
    "\n",
    "features_list = []\n",
    "\n",
    "for idx, frame in enumerate(selected_frames):\n",
    "    # Crop the frame to remove the sides\n",
    "    cropped_image = crop_image_sides(frame, pixels=80)\n",
    "    cv2.imwrite(f\"{CROPPED_FRAMES}/frame_{idx}.png\", cropped_image)\n",
    "    # Resize frame for InceptionV3\n",
    "    frame_resized = cv2.resize(cropped_image, INCEPTION_SIZE)\n",
    "    cv2.imwrite(f\"{RESIZED_FRAMES}/frame_{idx}.png\", frame_resized)\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image_tensor = tf.convert_to_tensor(frame_rgb, dtype=tf.float32)\n",
    "    print(f\"Preprocessed frame shape: {image_tensor.shape}\")\n",
    "\n",
    "    preprocessed_frame = np.expand_dims(image_tensor, axis=0)\n",
    "\n",
    "    features = feature_extractor.predict(preprocessed_frame)\n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "\n",
    "    features_list.append(features[0])\n",
    "\n",
    "if not features_list:\n",
    "    raise ValueError(\"No valid frames were processed.\")\n",
    "\n",
    "features_array = np.array(features_list, dtype='float32')\n",
    "print(\"Captured features shape:\", features_array.shape)\n",
    "\n",
    "input_for_lstm = np.expand_dims(features_array, axis=0)\n",
    "\n",
    "# Inference\n",
    "input_data = input_for_lstm.astype(input_details[0]['dtype'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "predicted_probabilities = output_data[0]\n",
    "\n",
    "print(\"Predicted probabilities:\")\n",
    "for i, prob in enumerate(predicted_probabilities):\n",
    "    print(f\"{index_to_label[i]}: {prob:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
